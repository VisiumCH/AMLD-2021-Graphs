{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('env')",
   "metadata": {
    "interpreter": {
     "hash": "3f9a800dfd4937aaf5915b9b3651c6a325ec28e4e6583e144cef767ca01f35a9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interact_manual\n",
    "import pandas as pd\n",
    "\n",
    "elements = pd.read_csv(\"/io/data/external/elementlist.csv\", index_col=0)"
   ]
  },
  {
   "source": [
    "# Molecules classification\n",
    "\n",
    "## The dataset\n",
    "\n",
    "We study the [`ogbg-molhiv` dataset][1].\n",
    "Each graph represents a molecule, where nodes are atoms, and edges are chemical bonds. Input node features are 9-dimensional, containing atomic number and chirality, as well as other additional atom features such as formal charge and whether the atom is in the ring or not.\n",
    "\n",
    "## The task\n",
    "\n",
    "We want to predict whether a molecule inhibits HIV virus replication or not, as accurately as possible,\n",
    "\n",
    "[1]: https://ogb.stanford.edu/docs/graphprop/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from ogb.graphproppred import PygGraphPropPredDataset # Dataset package\n",
    "from torch_geometric.data import DataLoader # Utility to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and process data at './dataset/ogbg_molhiv/'\n",
    "dataset = PygGraphPropPredDataset(name = \"ogbg-molhiv\", root = '/io/data/raw')"
   ]
  },
  {
   "source": [
    "### Features\n",
    "\n",
    "From the [doc](https://github.com/snap-stanford/ogb/blob/master/ogb/utils/features.py).\n",
    "\n",
    "**Atom** (node) features:\n",
    "- atomic_num : cat (118 vals)\n",
    "- chirality : cat (4 vals)\n",
    "- degree : int (0 to 10)\n",
    "- formal_charge : int (-5 to 5)\n",
    "- numH : int (0 to 8)\n",
    "- number_radical_e : int (0 to 4)\n",
    "- possible hybridization : cat (5 vals)\n",
    "- is_aromatic : bool\n",
    "- is_in_ring : bool\n",
    "\n",
    "**Bond** (edge) features:\n",
    "- bond_type : cat (4 vals)\n",
    "- bond_stereo : cat (6 vals)\n",
    "- is_conjugated : bool"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nFirst element : Data(edge_attr=[40, 3], edge_index=[2, 40], x=[19, 9], y=[1, 1])\n\nAttributes : \n    - Node features : 9\n    - Edge features : 3\n\nelements in el0.x[0] : ['C', 'C', 'C', 'O', 'Cu', 'O', 'C', 'C', 'C', 'C', 'O', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'O']\n\n"
     ]
    }
   ],
   "source": [
    "el0 = dataset[0]\n",
    "print(f\"\"\"\n",
    "First element : {el0}\n",
    "\n",
    "Attributes : \n",
    "    - Node features : {el0.num_node_features}\n",
    "    - Edge features : {el0.num_edge_features}\n",
    "\n",
    "elements in el0.x[0] : {[elements.iloc[el0.x[i,0].item()].symbol for i in range(el0.num_nodes)]}\n",
    "\"\"\")"
   ]
  },
  {
   "source": [
    "## Visualization\n",
    "\n",
    "We will use a graph visualization library to represent the molecules.\n",
    "\n",
    "_NOTE: consider writing interactively the basic plot function:_\n",
    "```\n",
    "    torch_graph -> networkx.Graph\n",
    "    networkx.draw(networkx.Graph)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "from src.visualization import plot_mol  #refactored function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "interactive(children=(Dropdown(description='x', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6499918514c747d79eac971ef532f2b4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x, layout)>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "interact(\n",
    "    lambda x, layout: plot_mol(dataset[x], layout),\n",
    "    x=range(len(dataset)), layout=[\"kamada_kawai\", \"spring\"]\n",
    ")"
   ]
  },
  {
   "source": [
    "## Graph neural networks\n",
    "\n",
    "We will use the [**Pytorch Geometric**][1] library to define a message passing network and use it for classification.\n",
    "\n",
    "_NOTE: One should use this notebook to explain some backgroud on graph convolutions_\n",
    "\n",
    "[1]: https://pytorch-geometric.readthedocs.io/en/latest/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import torch  # Computational module\n",
    "\n",
    "from torch.nn import Linear, NLLLoss, Sigmoid, LogSoftmax\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool"
   ]
  },
  {
   "source": [
    "### Model definition\n",
    "\n",
    "Let's start with an easy example.\n",
    "We will use on of the most simple GNN operators, the **GCN layer** ([Kipf et al. (2017)](https://arxiv.org/abs/1609.02907)), which is defined as\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_v^{(\\ell + 1)} = \\mathbf{W}^{(\\ell + 1)} \\sum_{w \\in \\mathcal{N}(v) \\, \\cup \\, \\{ v \\}} \\frac{1}{c_{w,v}} \\cdot \\mathbf{x}_w^{(\\ell)}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}^{(\\ell + 1)}$ denotes a trainable weight matrix of shape `[num_output_features, num_input_features]` and $c_{w,v}$ refers to a fixed normalization coefficient for each edge.\n",
    "\n",
    "PyG implements this layer via [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv), which can be executed by passing in the node feature representation `x` and the COO graph connectivity representation `edge_index`.\n",
    "\n",
    "\n",
    "_Note_ that the features of each atom need to be embedded prior to using them.\n",
    "For this we just use the functions provided by the OGB package"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.graphproppred.mol_encoder import AtomEncoder, BondEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels,\n",
    "        hidden_nodes,\n",
    "        atom_emb_dim=dataset.num_node_features,\n",
    "        bond_emb_dim=dataset.num_edge_features\n",
    "    ):\n",
    "        super(GNN, self).__init__()\n",
    "        self.atom_encoder = AtomEncoder(emb_dim = atom_emb_dim)\n",
    "        self.bond_encoder = BondEncoder(emb_dim = bond_emb_dim)\n",
    "        \n",
    "        self.conv1 = GCNConv(atom_emb_dim, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_nodes)\n",
    "        self.lin = Linear(hidden_nodes, dataset.num_classes)\n",
    "        self.readout = LogSoftmax(dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # 1. Obtain node embeddings\n",
    "        x = x.type(torch.FloatTensor)\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = x.relu()\n",
    "\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, data.batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return self.readout(x)"
   ]
  },
  {
   "source": [
    "### Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_tr, idx_val, idx_te = dataset.get_idx_split().values()\n",
    "\n",
    "# NOTE: time per epoch by batch_size\n",
    "# 128 -> 5.35s\n",
    "# 256 -> 5.20s\n",
    "batch_size = 128\n",
    "\n",
    "loader_tr = DataLoader(dataset[idx_tr], batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(dataset[idx_val], batch_size=batch_size, shuffle=True)\n",
    "loader_te = DataLoader(dataset[idx_te], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = GNN(\n",
    "    hidden_channels=64,\n",
    "    hidden_nodes=64\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters())\n",
    "loss_function = NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/200 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b00d6be1b50d4078b5be9ff6453646ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/258 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f7d7ceab37d4d6289bee7a3e24efb2a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 4 were given",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e406cfec7ed7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrain_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mval_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-bf89df780d2f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate in batches over the training/test dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m          \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m          \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use the class with highest probability.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m          \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Check against ground-truth labels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AMLD-2021-Graphs/env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "train_accs = []\n",
    "val_accs = []\n",
    "for epoch in tqdm(range(200)):\n",
    "    for batch in tqdm(loader_tr, leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch)\n",
    "        loss = loss_function(pred, batch.y.flatten())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_accs.append(test(loader_tr))\n",
    "        val_accs.append(test(loader_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the model for future usage\n",
    "\n",
    "torch.save(model.state_dict(), \"/io/models/mol_classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING VISUALIZATION\n",
    "\n",
    "fig, ax = plt.subplots(dpi=120)\n",
    "\n",
    "ax.plot(train_accs, c=\"steelblue\", label=\"Training\")\n",
    "ax.plot(val_accs, c=\"orangered\", label=\"Validation\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "As multiple papers pointed out ([Xu et al. (2018)](https://arxiv.org/abs/1810.00826), [Morris et al. (2018)](https://arxiv.org/abs/1810.02244)), applying **neighborhood normalization decreases the expressivity of GNNs in distinguishing certain graph structures**.\n",
    "An alternative formulation ([Morris et al. (2018)](https://arxiv.org/abs/1810.02244)) omits neighborhood normalization completely and adds a simple skip-connection to the GNN layer in order to preserve central node information:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_v^{(\\ell+1)} = \\mathbf{W}^{(\\ell + 1)}_1 \\mathbf{x}_v^{(\\ell)} + \\mathbf{W}^{(\\ell + 1)}_2 \\sum_{w \\in \\mathcal{N}(v)} \\mathbf{x}_w^{(\\ell)}\n",
    "$$\n",
    "\n",
    "This layer is implemented under the name [`GraphConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GraphConv) in PyTorch Geometric."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/33 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7641204229dd463aaba0944afdaec294"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = torch.cat(\n",
    "        [model(batch.to(device)) for batch in tqdm(loader_te)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-7.3637e-03, -4.9149e+00],\n",
       "        [-8.1757e-03, -4.8107e+00],\n",
       "        [-1.7376e-02, -4.0613e+00],\n",
       "        ...,\n",
       "        [-1.3251e-02, -4.3303e+00],\n",
       "        [-1.9358e-02, -3.9543e+00],\n",
       "        [-3.8339e-03, -5.5658e+00]])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==== Expected input format of Evaluator for ogbg-molhiv\n{'y_true': y_true, 'y_pred': y_pred}\n- y_true: numpy ndarray or torch tensor of shape (num_graph, num_task)\n- y_pred: numpy ndarray or torch tensor of shape (num_graph, num_task)\nwhere y_pred stores score values (for computing AUC score),\nnum_task is 1, and each row corresponds to one graph.\nnan values in y_true are ignored during evaluation.\n\n==== Expected output format of Evaluator for ogbg-molhiv\n{'rocauc': rocauc}\n- rocauc (float): ROC-AUC score averaged across 1 task(s)\n\n"
     ]
    }
   ],
   "source": [
    "from ogb.graphproppred import Evaluator\n",
    "\n",
    "evaluator = Evaluator(name = \"ogbg-molhiv\")\n",
    "print(evaluator.expected_input_format) \n",
    "print(evaluator.expected_output_format)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In most cases, input_dict is\n",
    "y_true = torch.cat(\n",
    "    [batch.y for batch in loader_te]\n",
    ")\n",
    "input_dict = {\n",
    "    \"y_true\": y_true,\n",
    "    \"y_pred\": torch.stack([y_pred[i, correct] for i, correct in enumerate(y_true)])\n",
    "}\n",
    "\n",
    "result_dict = evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'rocauc': 0.0}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[3983,    0],\n",
       "       [ 130,    0]])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "confusion_matrix(y_true.flatten(), y_pred.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}