{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Explainer\n",
    "\n",
    "The purpose of this notebook is to present GNN Explainer, a general, model-agnostic approach for providing inter-\n",
    "pretable explanations for predictions of any GNN-based model on any graph-based\n",
    "machine learning task.\n",
    "\n",
    "The notebook is organized as follows:\n",
    "\n",
    "* Brief theoretical recap on GNN-EXPLAINER\n",
    "* Train your GNN-EXPLAINER to explain graph classification predictions\n",
    "* Visualize and understand the propose explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your GNN Explainer!\n",
    "\n",
    "In this section we will build together the main module used by the explainer, which is implemented in the **ExplainModule class**. This class contains several methods, in particular:\n",
    "\n",
    "**construct_feat_mask**: initializes the feature mask that will be learned by the explainer\n",
    "\n",
    "**construct edge_mask**: initializes the edge mask that will be learne by the explainer\n",
    "\n",
    "**mask_adj**: computes the masked adjacency matrices of the graph whose prediction we want to explain\n",
    "\n",
    "**mask_density**: computes mask density as (sum masked entried)/(original sum of entries)\n",
    "\n",
    "**forward**: returns the model prediction (and edge attention, if available), based on the current edge and feature masks.\n",
    "\n",
    "**loss**: computes the loss function as explained in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorboardX.utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from src.utils import io_utils\n",
    "import src.GNNtrainer.models as models\n",
    "from src.GNNexplainer.explainer import Explainer, train_explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainModule(nn.Module):\n",
    "    def __init__(\n",
    "        self, adj, x, model, label, args, graph_idx=0, writer=None, use_sigmoid=True\n",
    "    ):\n",
    "        super(ExplainModule, self).__init__()\n",
    "        self.adj = adj\n",
    "        self.x = x\n",
    "        self.model = model\n",
    "        self.label = label\n",
    "        self.graph_idx = graph_idx\n",
    "        self.args = args\n",
    "        self.writer = writer\n",
    "        self.mask_act = args.mask_act\n",
    "        self.use_sigmoid = use_sigmoid\n",
    "        self.graph_mode = True\n",
    "        # Relative weights for the terms in the loss function.\n",
    "        self.coeffs = {\n",
    "            \"size\": 0.005,\n",
    "            \"feat_size\": 1.0,\n",
    "            \"ent\": 1.0,\n",
    "            \"feat_ent\": 0.1,\n",
    "            \"grad\": 0,\n",
    "            \"lap\": 1.0,\n",
    "        }\n",
    "        num_nodes = adj.size()[1]\n",
    "        init_strategy = \"normal\"\n",
    "        # Initialize the edge mask to be optimized.\n",
    "        self.mask, self.mask_bias = self.construct_edge_mask(\n",
    "            num_nodes, init_strategy=init_strategy\n",
    "        )\n",
    "        # Initialize the feature mask to be optimized.\n",
    "        self.feat_mask = self.construct_feat_mask(x.size(-1), init_strategy=\"constant\")\n",
    "        params = [self.mask, self.feat_mask]\n",
    "        if self.mask_bias is not None:\n",
    "            params.append(self.mask_bias)\n",
    "        # For masking diagonal entries.\n",
    "        self.diag_mask = torch.ones(num_nodes, num_nodes) - torch.eye(num_nodes)\n",
    "        if args.gpu:\n",
    "            self.diag_mask = self.diag_mask.cuda()\n",
    "\n",
    "        self.scheduler, self.optimizer = train_utils.build_optimizer(args, params)\n",
    "\n",
    "    def construct_feat_mask(self, feat_dim, init_strategy=\"normal\"):\n",
    "        \"\"\"Initialize the feature mask. init_strategy is a string specifying\n",
    "        the chosen initialization strategy (can be 'costant' or 'normal'.)\n",
    "        \"\"\"\n",
    "        mask = nn.Parameter(torch.FloatTensor(feat_dim))\n",
    "        if init_strategy == \"normal\":\n",
    "            std = 0.1\n",
    "            with torch.no_grad():\n",
    "                mask.normal_(1.0, std)\n",
    "        elif init_strategy == \"constant\":\n",
    "            with torch.no_grad():\n",
    "                nn.init.constant_(mask, 0.0)\n",
    "        return mask\n",
    "\n",
    "    def construct_edge_mask(self, num_nodes, init_strategy=\"normal\", const_val=1.0):\n",
    "        \"\"\"Initialize the edge mask. init_strategy is a string specifying\n",
    "        the chosen initialization strategy (eg 'costant' or 'normal'.)\n",
    "        \"\"\"\n",
    "        mask = nn.Parameter(torch.FloatTensor(num_nodes, num_nodes))\n",
    "        if init_strategy == \"normal\":\n",
    "            std = nn.init.calculate_gain(\"relu\") * math.sqrt(\n",
    "                2.0 / (num_nodes + num_nodes)\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                mask.normal_(1.0, std)\n",
    "        elif init_strategy == \"const\":\n",
    "            nn.init.constant_(mask, const_val)\n",
    "        if self.args.mask_bias:\n",
    "            mask_bias = nn.Parameter(torch.FloatTensor(num_nodes, num_nodes))\n",
    "            nn.init.constant_(mask_bias, 0.0)\n",
    "        else:\n",
    "            mask_bias = None\n",
    "\n",
    "        return mask, mask_bias\n",
    "\n",
    "    def _masked_adj(self):\n",
    "        \"\"\"Computes the masked adjacency matrix of the graph. Since\n",
    "        we work with undirected graphs, we make the mask symmetric.\n",
    "        Self-loops are also excluded using a diagonal mask.\n",
    "        \"\"\"\n",
    "        sym_mask = self.mask\n",
    "        if self.mask_act == \"sigmoid\":\n",
    "            sym_mask = torch.sigmoid(self.mask)\n",
    "        elif self.mask_act == \"ReLU\":\n",
    "            sym_mask = nn.ReLU()(self.mask)\n",
    "        sym_mask = (sym_mask + sym_mask.t()) / 2\n",
    "        adj = self.adj.cuda() if self.args.gpu else self.adj\n",
    "        masked_adj = adj * sym_mask\n",
    "        if self.args.mask_bias:\n",
    "            bias = (self.mask_bias + self.mask_bias.t()) / 2\n",
    "            bias = nn.ReLU6()(bias * 6) / 6\n",
    "            masked_adj += (bias + bias.t()) / 2\n",
    "        return masked_adj * self.diag_mask\n",
    "\n",
    "    def mask_density(self):\n",
    "        mask_sum = torch.sum(self._masked_adj()).cpu()\n",
    "        adj_sum = torch.sum(self.adj)\n",
    "        return mask_sum / adj_sum\n",
    "\n",
    "    def forward(self, mask_features=True, marginalize=False):\n",
    "        \"\"\"Computes the model prediction on the masked graph with masked features.\n",
    "        Returns the model predictions and adjacency attention (if available).\n",
    "        \"\"\"\n",
    "        x = self.x.cuda() if self.args.gpu else self.x\n",
    "        self.masked_adj = self._masked_adj()\n",
    "        if mask_features:\n",
    "            feat_mask = (\n",
    "                torch.sigmoid(self.feat_mask) if self.use_sigmoid else self.feat_mask\n",
    "            )\n",
    "            if marginalize:\n",
    "                std_tensor = torch.ones_like(x, dtype=torch.float) / 2\n",
    "                mean_tensor = torch.zeros_like(x, dtype=torch.float) - x\n",
    "                z = torch.normal(mean=mean_tensor, std=std_tensor)\n",
    "                x = x + z * (1 - feat_mask)\n",
    "            else:\n",
    "                x = x * feat_mask\n",
    "\n",
    "        ypred, adj_att = self.model(x, self.masked_adj)\n",
    "        res = nn.Softmax(dim=0)(ypred[0])\n",
    "\n",
    "        return res, adj_att\n",
    "\n",
    "    def loss(self, pred, epoch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: prediction made by current model (with current mask).\n",
    "            epoch: training epoch.\n",
    "        \"\"\"\n",
    "        # Prediction loss.\n",
    "        gt_label = self.label\n",
    "        logit = pred[gt_label]\n",
    "        pred_loss = -torch.log(logit)\n",
    "        # Adjacency mask size loss.\n",
    "        mask = self.mask\n",
    "        if self.mask_act == \"sigmoid\":\n",
    "            mask = torch.sigmoid(self.mask)\n",
    "        elif self.mask_act == \"ReLU\":\n",
    "            mask = nn.ReLU()(self.mask)\n",
    "        size_loss = self.coeffs[\"size\"] * torch.sum(mask)\n",
    "        # Feature mask size loss.\n",
    "        feat_mask = (\n",
    "            torch.sigmoid(self.feat_mask) if self.use_sigmoid else self.feat_mask\n",
    "        )\n",
    "        feat_size_loss = self.coeffs[\"feat_size\"] * torch.mean(feat_mask)\n",
    "        # Adjacency mask entropy loss.\n",
    "        mask_ent = -mask * torch.log(mask) - (1 - mask) * torch.log(1 - mask)\n",
    "        mask_ent_loss = self.coeffs[\"ent\"] * torch.mean(mask_ent)\n",
    "        # Feature mask entropy loss.\n",
    "        feat_mask_ent = -feat_mask * torch.log(feat_mask) - (1 - feat_mask) * torch.log(\n",
    "            1 - feat_mask\n",
    "        )\n",
    "        feat_mask_ent_loss = self.coeffs[\"feat_ent\"] * torch.mean(feat_mask_ent)\n",
    "        # Total loss.\n",
    "        loss = pred_loss + size_loss + mask_ent_loss + feat_size_loss\n",
    "\n",
    "        # Log data to tensorboard.\n",
    "        if self.writer is not None:\n",
    "            self.writer.add_scalar(\"optimization/size_loss\", size_loss, epoch)\n",
    "            self.writer.add_scalar(\"optimization/feat_size_loss\", feat_size_loss, epoch)\n",
    "            self.writer.add_scalar(\"optimization/mask_ent_loss\", mask_ent_loss, epoch)\n",
    "            self.writer.add_scalar(\n",
    "                \"optimization/feat_mask_ent_loss\", mask_ent_loss, epoch\n",
    "            )\n",
    "            self.writer.add_scalar(\"optimization/pred_loss\", pred_loss, epoch)\n",
    "            # self.writer.add_scalar(\"optimization/lap_loss\", lap_loss, epoch)\n",
    "            self.writer.add_scalar(\"optimization/overall_loss\", loss, epoch)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our explainer, we then use the trainExplainer function, implemented in the explainer.py file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "\n",
    "import src.GNNexplainer.configs_explainer as configs_explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple('Args', 'logdir ckptdir dataset opt opt_scheduler cuda lr clip batch_size num_epochs hidden_dim output_dim gpu bmname method bias name_suffix explainer_suffix num_gc_layers bn graph_idx mask_act mask_bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog_args = Args(logdir=\"/io/log\",\n",
    "        ckptdir=\"/io/ckpt\",\n",
    "        dataset=\"REDDIT-BINARY\",\n",
    "        opt=\"adam\",\n",
    "        opt_scheduler=\"none\",\n",
    "        cuda=\"0\",\n",
    "        lr=0.1,\n",
    "        clip=2.0,\n",
    "        batch_size=20,\n",
    "        num_epochs=100,\n",
    "        hidden_dim=20,\n",
    "        output_dim=20,\n",
    "                gpu=False,\n",
    "                bmname=None,\n",
    "                method = 'base',\n",
    "                bias='True', name_suffix =\"\", explainer_suffix=\"\", num_gc_layers=3, bn=False, graph_idx=2, mask_act = 'sigmoid', mask_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "# Load a configuration\n",
    "if prog_args.gpu:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = prog_args.cuda\n",
    "    print(\"CUDA\", prog_args.cuda)\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/io/log/REDDIT-BINARY_base_h20_o20_explain\n"
     ]
    }
   ],
   "source": [
    "# Configure the logging directory\n",
    "path = os.path.join(prog_args.logdir, io_utils.gen_explainer_prefix(prog_args))\n",
    "print(path)\n",
    "writer = SummaryWriter(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "/io/ckpt/REDDIT-BINARY_base_h20_o20.pth.tar\n",
      "=> loading checkpoint '/io/ckpt/REDDIT-BINARY_base_h20_o20.pth.tar'\n",
      "Loaded model from /io/ckpt\n",
      "input dim:  10 ; num classes:  2\n"
     ]
    }
   ],
   "source": [
    "# Load a model checkpoint\n",
    "ckpt = io_utils.load_ckpt(prog_args)\n",
    "cg_dict = ckpt[\"cg\"]  # get computation graph\n",
    "input_dim = cg_dict[\"feat\"].shape[2]\n",
    "num_classes = cg_dict[\"pred\"].shape[2]\n",
    "print(\"Loaded model from {}\".format(prog_args.ckptdir))\n",
    "print(\"input dim: \", input_dim, \"; num classes: \", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method:  base\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "print(\"Method: \", prog_args.method)\n",
    "\n",
    "# Explain Graph prediction\n",
    "model = models.GcnEncoderGraph(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=prog_args.hidden_dim,\n",
    "    embedding_dim=prog_args.output_dim,\n",
    "    label_dim=num_classes,\n",
    "    num_layers=prog_args.num_gc_layers,\n",
    "    bn=prog_args.bn,\n",
    "    args=prog_args,\n",
    ")\n",
    "if prog_args.gpu:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/camilla.casamento/AMLD-2021-Graphs/env/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# Load state_dict (obtained by model.state_dict() when saving checkpoint)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model = model.eval()\n",
    "\n",
    "# Extract the data relative to the chosen graph.\n",
    "adj = cg_dict[\"adj\"]\n",
    "feat = cg_dict[\"feat\"]\n",
    "label = cg_dict[\"label\"]\n",
    "pred = cg_dict[\"pred\"]\n",
    "\n",
    "graph_idx = prog_args.graph_idx\n",
    "sub_adj = adj[graph_idx]\n",
    "sub_feat = feat[graph_idx, :]\n",
    "sub_label = label[graph_idx]\n",
    "neighbors = np.asarray(range(adj.shape[0]))\n",
    "\n",
    "sub_adj = np.expand_dims(sub_adj, axis=0)\n",
    "sub_feat = np.expand_dims(sub_feat, axis=0)\n",
    "\n",
    "adj = torch.tensor(sub_adj, dtype=torch.float)\n",
    "x = torch.tensor(sub_feat, requires_grad=True, dtype=torch.float)\n",
    "label = torch.tensor(sub_label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph predicted label:  [ 13 176]\n",
      "epoch:  0 ; loss:  37.79560089111328 ; mask density:  0.7128463983535767 ; pred:  tensor([0.8284, 0.1716], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  1 ; loss:  36.78696823120117 ; mask density:  0.6920223236083984 ; pred:  tensor([0.8285, 0.1715], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  2 ; loss:  35.73348617553711 ; mask density:  0.6703581809997559 ; pred:  tensor([0.8285, 0.1715], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  3 ; loss:  34.63710021972656 ; mask density:  0.6479053497314453 ; pred:  tensor([0.8285, 0.1715], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  4 ; loss:  33.50037384033203 ; mask density:  0.6247288584709167 ; pred:  tensor([0.8285, 0.1715], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  5 ; loss:  32.32658004760742 ; mask density:  0.6009079813957214 ; pred:  tensor([0.8285, 0.1715], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  6 ; loss:  31.11968231201172 ; mask density:  0.5765355229377747 ; pred:  tensor([0.8286, 0.1714], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  7 ; loss:  29.884349822998047 ; mask density:  0.5517178177833557 ; pred:  tensor([0.8286, 0.1714], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  8 ; loss:  28.625930786132812 ; mask density:  0.5265734791755676 ; pred:  tensor([0.8286, 0.1714], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  9 ; loss:  27.350385665893555 ; mask density:  0.50123131275177 ; pred:  tensor([0.8287, 0.1713], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  10 ; loss:  26.064212799072266 ; mask density:  0.4758283197879791 ; pred:  tensor([0.8287, 0.1713], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  11 ; loss:  24.774328231811523 ; mask density:  0.45050689578056335 ; pred:  tensor([0.8288, 0.1712], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  12 ; loss:  23.48792839050293 ; mask density:  0.42541155219078064 ; pred:  tensor([0.8289, 0.1711], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  13 ; loss:  22.21234130859375 ; mask density:  0.4006853401660919 ; pred:  tensor([0.8289, 0.1711], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  14 ; loss:  20.954824447631836 ; mask density:  0.3764662444591522 ; pred:  tensor([0.8290, 0.1710], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  15 ; loss:  19.722471237182617 ; mask density:  0.35289156436920166 ; pred:  tensor([0.8291, 0.1709], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  16 ; loss:  18.521814346313477 ; mask density:  0.33007776737213135 ; pred:  tensor([0.8291, 0.1709], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  17 ; loss:  17.35886573791504 ; mask density:  0.30812785029411316 ; pred:  tensor([0.8292, 0.1708], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  18 ; loss:  16.23890495300293 ; mask density:  0.2871280908584595 ; pred:  tensor([0.8292, 0.1708], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  19 ; loss:  15.166402816772461 ; mask density:  0.26714757084846497 ; pred:  tensor([0.8293, 0.1707], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  20 ; loss:  14.144889831542969 ; mask density:  0.24823617935180664 ; pred:  tensor([0.8293, 0.1707], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  21 ; loss:  13.176979064941406 ; mask density:  0.23042583465576172 ; pred:  tensor([0.8294, 0.1706], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  22 ; loss:  12.26434326171875 ; mask density:  0.21373426914215088 ; pred:  tensor([0.8295, 0.1705], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  23 ; loss:  11.407793045043945 ; mask density:  0.1981596052646637 ; pred:  tensor([0.8295, 0.1705], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  24 ; loss:  10.607246398925781 ; mask density:  0.1836806982755661 ; pred:  tensor([0.8295, 0.1705], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  25 ; loss:  9.861848831176758 ; mask density:  0.17026306688785553 ; pred:  tensor([0.8296, 0.1704], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  26 ; loss:  9.16992473602295 ; mask density:  0.15782661736011505 ; pred:  tensor([0.8299, 0.1701], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  27 ; loss:  8.529595375061035 ; mask density:  0.1463259756565094 ; pred:  tensor([0.8305, 0.1695], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  28 ; loss:  7.938681125640869 ; mask density:  0.13571204245090485 ; pred:  tensor([0.8312, 0.1688], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  29 ; loss:  7.394521236419678 ; mask density:  0.12591680884361267 ; pred:  tensor([0.8320, 0.1680], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  30 ; loss:  6.894543170928955 ; mask density:  0.1169135645031929 ; pred:  tensor([0.8329, 0.1671], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  31 ; loss:  6.435815334320068 ; mask density:  0.10864003747701645 ; pred:  tensor([0.8338, 0.1662], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  32 ; loss:  6.01540994644165 ; mask density:  0.10102156549692154 ; pred:  tensor([0.8347, 0.1653], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  33 ; loss:  5.630221366882324 ; mask density:  0.09398355334997177 ; pred:  tensor([0.8358, 0.1642], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  34 ; loss:  5.277739524841309 ; mask density:  0.08751701563596725 ; pred:  tensor([0.8370, 0.1630], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  35 ; loss:  4.955803394317627 ; mask density:  0.08163270354270935 ; pred:  tensor([0.8376, 0.1624], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  36 ; loss:  4.661665439605713 ; mask density:  0.07629496604204178 ; pred:  tensor([0.8380, 0.1620], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  37 ; loss:  4.392691135406494 ; mask density:  0.0714290663599968 ; pred:  tensor([0.8383, 0.1617], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  38 ; loss:  4.146493434906006 ; mask density:  0.06700856238603592 ; pred:  tensor([0.8387, 0.1613], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  39 ; loss:  3.9213902950286865 ; mask density:  0.06302004307508469 ; pred:  tensor([0.8387, 0.1613], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  40 ; loss:  3.715306282043457 ; mask density:  0.05939627066254616 ; pred:  tensor([0.8387, 0.1613], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  41 ; loss:  3.5261504650115967 ; mask density:  0.05610833689570427 ; pred:  tensor([0.8389, 0.1611], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  42 ; loss:  3.3525781631469727 ; mask density:  0.05310209468007088 ; pred:  tensor([0.8390, 0.1610], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  43 ; loss:  3.1929399967193604 ; mask density:  0.05035935342311859 ; pred:  tensor([0.8392, 0.1608], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  44 ; loss:  3.0462093353271484 ; mask density:  0.047875624150037766 ; pred:  tensor([0.8393, 0.1607], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  45 ; loss:  2.9112353324890137 ; mask density:  0.04564037173986435 ; pred:  tensor([0.8393, 0.1607], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  46 ; loss:  2.7867188453674316 ; mask density:  0.04361496493220329 ; pred:  tensor([0.8393, 0.1607], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  47 ; loss:  2.6717796325683594 ; mask density:  0.04179208353161812 ; pred:  tensor([0.8393, 0.1607], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  48 ; loss:  2.5654895305633545 ; mask density:  0.04014318063855171 ; pred:  tensor([0.8393, 0.1607], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  49 ; loss:  2.467261791229248 ; mask density:  0.038674116134643555 ; pred:  tensor([0.8391, 0.1609], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  50 ; loss:  2.3763365745544434 ; mask density:  0.03736982122063637 ; pred:  tensor([0.8388, 0.1612], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  51 ; loss:  2.291942596435547 ; mask density:  0.03622301295399666 ; pred:  tensor([0.8384, 0.1616], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  52 ; loss:  2.213562250137329 ; mask density:  0.035239461809396744 ; pred:  tensor([0.8380, 0.1620], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  53 ; loss:  2.1406033039093018 ; mask density:  0.03438634052872658 ; pred:  tensor([0.8376, 0.1624], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  54 ; loss:  2.0724692344665527 ; mask density:  0.03365892916917801 ; pred:  tensor([0.8373, 0.1627], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  55 ; loss:  2.0088579654693604 ; mask density:  0.03304729610681534 ; pred:  tensor([0.8370, 0.1630], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  56 ; loss:  1.9492884874343872 ; mask density:  0.0325419045984745 ; pred:  tensor([0.8368, 0.1632], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  57 ; loss:  1.8934288024902344 ; mask density:  0.03213546797633171 ; pred:  tensor([0.8366, 0.1634], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  58 ; loss:  1.8410096168518066 ; mask density:  0.031806547194719315 ; pred:  tensor([0.8365, 0.1635], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  59 ; loss:  1.7918059825897217 ; mask density:  0.031553659588098526 ; pred:  tensor([0.8363, 0.1637], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  60 ; loss:  1.7454774379730225 ; mask density:  0.031370215117931366 ; pred:  tensor([0.8361, 0.1639], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  61 ; loss:  1.7017844915390015 ; mask density:  0.03125028312206268 ; pred:  tensor([0.8360, 0.1640], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  62 ; loss:  1.6605265140533447 ; mask density:  0.031186332926154137 ; pred:  tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  63 ; loss:  1.6215176582336426 ; mask density:  0.031174274161458015 ; pred:  tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  64 ; loss:  1.5845668315887451 ; mask density:  0.03121054172515869 ; pred:  tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  65 ; loss:  1.5495214462280273 ; mask density:  0.03129168227314949 ; pred:  tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  66 ; loss:  1.5162392854690552 ; mask density:  0.03141278773546219 ; pred:  tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  67 ; loss:  1.484597086906433 ; mask density:  0.031561229377985 ; pred:  tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  68 ; loss:  1.4544724225997925 ; mask density:  0.031734831631183624 ; pred:  tensor([0.8360, 0.1640], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  69 ; loss:  1.4257524013519287 ; mask density:  0.03194114938378334 ; pred:  tensor([0.8361, 0.1639], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  70 ; loss:  1.3983519077301025 ; mask density:  0.0321778729557991 ; pred:  tensor([0.8362, 0.1638], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  71 ; loss:  1.3721379041671753 ; mask density:  0.032418686896562576 ; pred:  tensor([0.8364, 0.1636], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  72 ; loss:  1.346725344657898 ; mask density:  0.032671768218278885 ; pred:  tensor([0.8368, 0.1632], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  73 ; loss:  1.3223389387130737 ; mask density:  0.032927535474300385 ; pred:  tensor([0.8373, 0.1627], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  74 ; loss:  1.2990230321884155 ; mask density:  0.033189039677381516 ; pred:  tensor([0.8377, 0.1623], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  75 ; loss:  1.2767596244812012 ; mask density:  0.033459439873695374 ; pred:  tensor([0.8381, 0.1619], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  76 ; loss:  1.2554861307144165 ; mask density:  0.03373744338750839 ; pred:  tensor([0.8383, 0.1617], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  77 ; loss:  1.235107660293579 ; mask density:  0.03400716185569763 ; pred:  tensor([0.8385, 0.1615], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  78 ; loss:  1.2155603170394897 ; mask density:  0.034269459545612335 ; pred:  tensor([0.8386, 0.1614], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  79 ; loss:  1.196760892868042 ; mask density:  0.034524522721767426 ; pred:  tensor([0.8387, 0.1613], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  80 ; loss:  1.1786459684371948 ; mask density:  0.03474938124418259 ; pred:  tensor([0.8388, 0.1612], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  81 ; loss:  1.1611645221710205 ; mask density:  0.03494657948613167 ; pred:  tensor([0.8389, 0.1611], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  82 ; loss:  1.1442980766296387 ; mask density:  0.035117413848638535 ; pred:  tensor([0.8390, 0.1610], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  83 ; loss:  1.1279970407485962 ; mask density:  0.035280950367450714 ; pred:  tensor([0.8391, 0.1609], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  84 ; loss:  1.112259030342102 ; mask density:  0.03543761000037193 ; pred:  tensor([0.8392, 0.1608], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  85 ; loss:  1.0970393419265747 ; mask density:  0.03558729588985443 ; pred:  tensor([0.8392, 0.1608], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  86 ; loss:  1.082297444343567 ; mask density:  0.035728856921195984 ; pred:  tensor([0.8393, 0.1607], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  87 ; loss:  1.0680203437805176 ; mask density:  0.035860903561115265 ; pred:  tensor([0.8394, 0.1606], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  88 ; loss:  1.05414879322052 ; mask density:  0.03598488122224808 ; pred:  tensor([0.8394, 0.1606], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  89 ; loss:  1.0407103300094604 ; mask density:  0.03610045462846756 ; pred:  tensor([0.8395, 0.1605], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  90 ; loss:  1.0276472568511963 ; mask density:  0.0362171046435833 ; pred:  tensor([0.8396, 0.1604], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  91 ; loss:  1.0149797201156616 ; mask density:  0.036332543939352036 ; pred:  tensor([0.8396, 0.1604], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  92 ; loss:  1.0026363134384155 ; mask density:  0.03644551336765289 ; pred:  tensor([0.8397, 0.1603], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  93 ; loss:  0.9906227588653564 ; mask density:  0.03654354438185692 ; pred:  tensor([0.8399, 0.1601], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  94 ; loss:  0.978951096534729 ; mask density:  0.03662670776247978 ; pred:  tensor([0.8399, 0.1601], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  95 ; loss:  0.9676088094711304 ; mask density:  0.03670930489897728 ; pred:  tensor([0.8400, 0.1600], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  96 ; loss:  0.9565598368644714 ; mask density:  0.036792878061532974 ; pred:  tensor([0.8401, 0.1599], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  97 ; loss:  0.9457656741142273 ; mask density:  0.03692217543721199 ; pred:  tensor([0.8402, 0.1598], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  98 ; loss:  0.9352070093154907 ; mask density:  0.03707899898290634 ; pred:  tensor([0.8403, 0.1597], grad_fn=<SoftmaxBackward>)\n",
      "epoch:  99 ; loss:  0.9248943328857422 ; mask density:  0.037279341369867325 ; pred:  tensor([0.8405, 0.1595], grad_fn=<SoftmaxBackward>)\n",
      "finished training in  5.711545944213867\n",
      "Saved adjacency matrix to  masked_adj_REDDIT-BINARY_base_h20_o20_explaingraph_idx_2.npy\n"
     ]
    }
   ],
   "source": [
    "# Create explainer\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    adj=adj,\n",
    "    x=x,\n",
    "    label=label,\n",
    "    args=prog_args,\n",
    "    writer=writer\n",
    ")\n",
    "\n",
    "# Run explainer.\n",
    "train_explainer(\n",
    "    explainer=explainer, pred=pred, args=prog_args, graph_idx=prog_args.graph_idx\n",
    ")\n",
    "io_utils.plot_cmap_tb(writer, \"tab20\", 20, \"tab20_cmap\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to visualize the results of the GNN Explainer.\n",
    "\n",
    "Use it after one has trained the model using train.py, and has run the explainer optimization (explainer_main.py).\n",
    "The main purpose is to visualize the trained mask by interactively tuning the threshold. In many scientific applications, the explanation size is unknown a priori. This tool can help user visualize the selected subgraph, with respect to different values of the thresholds, and find the right size for a good explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuring the experiment you want to visualize. These values should match the configuration:\n",
    "\n",
    "> TODO: Unify configuration of experiments in yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = '../io/log/'\n",
    "expdir = 'REDDIT-BINARY_base_h20_o20_explain/masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logdir = '../../gnn-model-explainer/log/'\n",
    "#expdir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the produced masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['masked_adj_REDDIT-BINARY_base_h20_o20_explaingraph_idx_2.npy',\n",
       " 'masked_adj_REDDIT-BINARY_base_h20_o20_explaingraph_idx_2.npy-filt-.json',\n",
       " 'masked_adj_REDDIT-BINARY_base_h20_o20_explaingraph_idx_3.npy']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs = os.listdir(os.path.join(logdir, expdir))\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked_adj_REDDIT-BINARY_base_h20_o20_explaingraph_idx_2.npy\n",
      "masked_adj_REDDIT-BINARY_base_h20_o20_explaingraph_idx_3.npy\n"
     ]
    }
   ],
   "source": [
    "masks = []\n",
    "# This would print all the files and directories\n",
    "for file in dirs:\n",
    "    if file.split('.')[-1] == 'npy':\n",
    "        print(file)\n",
    "        masks.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility to save masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.readwrite import json_graph\n",
    "\n",
    "def save_mask(G, fname, fmt='json', suffix=''):\n",
    "    pth = os.path.join(logdir, expdir, fname+'-filt-'+suffix+'.'+fmt)\n",
    "    if fmt == 'json':\n",
    "        dt = json_graph.node_link_data(G)\n",
    "        with open(pth, 'w') as f:\n",
    "            json.dump(dt, f)\n",
    "    elif fmt == 'pdf':\n",
    "        plt.savefig(pth)\n",
    "    elif fmt == 'npy':\n",
    "        np.save(pth, nx.to_numpy_array(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_adjacency_full(mask, ax=None):\n",
    "    adj = np.load(os.path.join(logdir, expdir, mask), allow_pickle=True)\n",
    "    if ax is None:\n",
    "        plt.figure()\n",
    "        plt.imshow(adj);\n",
    "    else:\n",
    "        ax.imshow(adj)\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_adjacency_full(mask, ax=None):\n",
    "    adj = np.load(os.path.join(logdir, expdir, mask), allow_pickle=True)\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a62f217be4415c88108402876b26ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='thresh', max=1.5, min=-0.5), Output()), _dom_classes…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filt_adj = read_adjacency_full(masks[0])\n",
    "@interact\n",
    "def filter_adj(thresh=0.5):\n",
    "    filt_adj[filt_adj<thresh] = 0\n",
    "    return filt_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight-based threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cafc5f4a7bd4f818689909889c48d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='thresh', max=1.0, step=0.01), Output()), _dom_classe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EDIT THIS INDEX\n",
    "MASK_IDX = 0\n",
    "# EDIT THIS INDEX\n",
    "\n",
    "m = masks[MASK_IDX]\n",
    "adj = read_adjacency_full(m)\n",
    "\n",
    "\n",
    "@interact(thresh=widgets.FloatSlider(value=0.5, min=0.0, max=1.0, step=0.01))\n",
    "def plot_interactive(thresh=0.5):\n",
    "    filt_adj = read_adjacency_full(m)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n",
    "    plt.title(str(m));\n",
    "\n",
    "    # Full adjacency\n",
    "    ax1.set_title('Full Adjacency mask')\n",
    "    adj = show_adjacency_full(m, ax=ax1);\n",
    "    \n",
    "    # Filtered adjacency\n",
    "    filt_adj[filt_adj<thresh] = 0\n",
    "    ax2.set_title('Filtered Adjacency mask');\n",
    "    ax2.imshow(filt_adj);\n",
    "    \n",
    "    # Plot subgraph\n",
    "    ax3.set_title(\"Subgraph\")\n",
    "    G_ = nx.from_numpy_array(adj)\n",
    "    G  = nx.from_numpy_array(filt_adj)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    nx.draw(G, ax=ax3)\n",
    "    save_mask(G, fname=m, fmt='json')\n",
    "    \n",
    "    print(\"Removed {} edges -- K = {} remain.\".format(G_.number_of_edges()-G.number_of_edges(), G.number_of_edges()))\n",
    "    print(\"Removed {} nodes -- K = {} remain.\".format(G_.number_of_nodes()-G.number_of_nodes(), G.number_of_nodes()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
