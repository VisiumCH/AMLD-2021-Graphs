{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train OGBG-molhiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/graphproppred/csv_mol_download/hiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.00 GB: 100%|██████████| 3/3 [00:03<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/hiv.zip\n",
      "Processing...\n",
      "Loading necessary files...\n",
      "This might take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 9382/41127 [00:00<00:00, 93808.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41127/41127 [00:00<00:00, 89293.91it/s]\n",
      "  9%|▉         | 3797/41127 [00:00<00:01, 29831.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41127/41127 [00:00<00:00, 60886.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Download and process data at './dataset/ogbg_molhiv/'\n",
    "dataset = PygGraphPropPredDataset(name = \"ogbg-molhiv\", root = 'dataset/')\n",
    "\n",
    " \n",
    "split_idx = dataset.get_idx_split() \n",
    "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset,\n",
    "    model,\n",
    "    args,\n",
    "    same_feat=True,\n",
    "    val_dataset=None,\n",
    "    test_dataset=None,\n",
    "    writer=None,\n",
    "    mask_nodes=True,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=0.001\n",
    "    )\n",
    "    iter = 0\n",
    "    best_val_result = {\"epoch\": 0, \"loss\": 0, \"acc\": 0}\n",
    "    test_result = {\"epoch\": 0, \"loss\": 0, \"acc\": 0}\n",
    "    train_accs = []\n",
    "    train_epochs = []\n",
    "    best_val_accs = []\n",
    "    best_val_epochs = []\n",
    "    test_accs = []\n",
    "    test_epochs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        begin_time = time.time()\n",
    "        avg_loss = 0.0\n",
    "        model.train()\n",
    "        predictions = []\n",
    "        print(\"Epoch: \", epoch)\n",
    "        for batch_idx, data in enumerate(dataset):\n",
    "            model.zero_grad()\n",
    "            if batch_idx == 0:\n",
    "                prev_adjs = data[\"adj\"]\n",
    "                prev_feats = data[\"feats\"]\n",
    "                prev_labels = data[\"label\"]\n",
    "                all_adjs = prev_adjs\n",
    "                all_feats = prev_feats\n",
    "                all_labels = prev_labels\n",
    "            elif batch_idx < 20:\n",
    "                prev_adjs = data[\"adj\"]\n",
    "                prev_feats = data[\"feats\"]\n",
    "                prev_labels = data[\"label\"]\n",
    "                all_adjs = torch.cat((all_adjs, prev_adjs), dim=0)\n",
    "                all_feats = torch.cat((all_feats, prev_feats), dim=0)\n",
    "                all_labels = torch.cat((all_labels, prev_labels), dim=0)\n",
    "            adj = Variable(data[\"adj\"].float(), requires_grad=False).to(device)\n",
    "            h0 = Variable(data[\"feats\"].float(), requires_grad=False).to(device)\n",
    "            label = Variable(data[\"label\"].long()).to(device)\n",
    "            batch_num_nodes = data[\"num_nodes\"].int().numpy() if mask_nodes else None\n",
    "            assign_input = Variable(\n",
    "                data[\"assign_feats\"].float(), requires_grad=False\n",
    "            ).to(device)\n",
    "\n",
    "            ypred, att_adj = model(h0, adj, batch_num_nodes, assign_x=assign_input)\n",
    "            if batch_idx < 5:\n",
    "                predictions += ypred.cpu().detach().numpy().tolist()\n",
    "\n",
    "            if not args.method == \"soft-assign\" or not args.linkpred:\n",
    "                loss = model.loss(ypred, label)\n",
    "            else:\n",
    "                loss = model.loss(ypred, label, adj, batch_num_nodes)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
    "            optimizer.step()\n",
    "            iter += 1\n",
    "            avg_loss += loss\n",
    "\n",
    "        avg_loss /= batch_idx + 1\n",
    "        elapsed = time.time() - begin_time\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"loss/avg_loss\", avg_loss, epoch)\n",
    "            if args.linkpred:\n",
    "                writer.add_scalar(\"loss/linkpred_loss\", model.link_loss, epoch)\n",
    "        print(\"Avg loss: \", avg_loss, \"; epoch time: \", elapsed)\n",
    "        result = evaluate(\n",
    "            dataset, model, args, name=\"Train\", max_num_examples=100, device=device\n",
    "        )\n",
    "        train_accs.append(result[\"acc\"])\n",
    "        train_epochs.append(epoch)\n",
    "        if val_dataset is not None:\n",
    "            val_result = evaluate(\n",
    "                val_dataset, model, args, name=\"Validation\", device=device\n",
    "            )\n",
    "            val_accs.append(val_result[\"acc\"])\n",
    "        if val_result[\"acc\"] > best_val_result[\"acc\"] - 1e-7:\n",
    "            best_val_result[\"acc\"] = val_result[\"acc\"]\n",
    "            best_val_result[\"epoch\"] = epoch\n",
    "            best_val_result[\"loss\"] = avg_loss\n",
    "        if test_dataset is not None:\n",
    "            test_result = evaluate(\n",
    "                test_dataset, model, args, name=\"Test\", device=device\n",
    "            )\n",
    "            test_result[\"epoch\"] = epoch\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"acc/train_acc\", result[\"acc\"], epoch)\n",
    "            writer.add_scalar(\"acc/val_acc\", val_result[\"acc\"], epoch)\n",
    "            writer.add_scalar(\"loss/best_val_loss\", best_val_result[\"loss\"], epoch)\n",
    "            if test_dataset is not None:\n",
    "                writer.add_scalar(\"acc/test_acc\", test_result[\"acc\"], epoch)\n",
    "\n",
    "        print(\"Best val result: \", best_val_result)\n",
    "        best_val_epochs.append(best_val_result[\"epoch\"])\n",
    "        best_val_accs.append(best_val_result[\"acc\"])\n",
    "        if test_dataset is not None:\n",
    "            print(\"Test result: \", test_result)\n",
    "            test_epochs.append(test_result[\"epoch\"])\n",
    "            test_accs.append(test_result[\"acc\"])\n",
    "\n",
    "    matplotlib.style.use(\"seaborn\")\n",
    "    plt.switch_backend(\"agg\")\n",
    "    plt.figure()\n",
    "    plt.plot(train_epochs, math_utils.exp_moving_avg(train_accs, 0.85), \"-\", lw=1)\n",
    "    # MODIFIED HERE (COMMENTED)\n",
    "    # if test_dataset is not None:\n",
    "    #    plt.plot(best_val_epochs, best_val_accs, \"bo\", test_epochs, test_accs, \"go\")\n",
    "    #    plt.legend([\"train\", \"val\", \"test\"])\n",
    "    # else:\n",
    "    #    plt.plot(best_val_epochs, best_val_accs, \"bo\")\n",
    "    #    plt.legend([\"train\", \"val\"])\n",
    "    # plt.savefig(io_utils.gen_train_plt_name(args), dpi=600)\n",
    "    # plt.close()\n",
    "    # matplotlib.style.use(\"default\")\n",
    "\n",
    "    print(all_adjs.shape, all_feats.shape, all_labels.shape)\n",
    "\n",
    "    cg_data = {\n",
    "        \"adj\": all_adjs,\n",
    "        \"feat\": all_feats,\n",
    "        \"label\": all_labels,\n",
    "        \"pred\": np.expand_dims(predictions, axis=0),\n",
    "        \"train_idx\": list(range(len(dataset))),\n",
    "    }\n",
    "    io_utils.save_checkpoint(model, optimizer, args, num_epochs=-1, cg_dict=cg_data)\n",
    "    return model, val_accs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
